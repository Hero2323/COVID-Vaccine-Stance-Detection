{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import (\n",
    "    Dataset, \n",
    "    DataLoader\n",
    ")\n",
    "\n",
    "import math\n",
    "\n",
    "from transformers import  (\n",
    "    BertPreTrainedModel, \n",
    "    XLMRobertaConfig,\n",
    "    XLMRobertaTokenizer\n",
    ")\n",
    "\n",
    "from transformers.optimization import (\n",
    "    AdamW, \n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from scipy.special import softmax\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    matthews_corrcoef,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    average_precision_score,\n",
    ")\n",
    "\n",
    "from transformers.models.xlm_roberta.modeling_xlm_roberta import (\n",
    "    XLMRobertaClassificationHead,\n",
    "    XLMRobertaConfig,\n",
    "    XLMRobertaModel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'xlm-roberta-base'\n",
    "num_labels = 3\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "train_batch_size = 8\n",
    "test_batch_size = 8\n",
    "warmup_ratio = 0.06\n",
    "weight_decay=0.0\n",
    "gradient_accumulation_steps = 1\n",
    "num_train_epochs = 25\n",
    "learning_rate = 1e-05\n",
    "adam_epsilon = 1e-08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_path, tokenizer, category=False):\n",
    "        dataset = pd.read_pickle(dataset_path)\n",
    "        self.data = dataset['tokens'].apply(lambda x: ' '.join(x)).values\n",
    "        max_len = 0\n",
    "        for i in tqdm(range(len(self.data))):\n",
    "            input_ids = tokenizer.encode(self.data[i], add_special_tokens=True)\n",
    "            max_len = max(max_len, len(input_ids))\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        if category:\n",
    "            self.targets = dataset['category']\n",
    "        else:\n",
    "            self.targets = dataset['stance']\n",
    "        del dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        data = self.data[item]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            data,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(self.targets[item], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a classification head based on Roberta\n",
    "class XLMRobertaClassifier(BertPreTrainedModel):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(XLMRobertaClassifier, self).__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.xlm_roberta = XLMRobertaModel(config)\n",
    "        self.classifier = XLMRobertaClassificationHead(config)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        outputs = self.xlm_roberta(input_ids,attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "        \n",
    "        loss_fct = CrossEntropyLoss()\n",
    "        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_class = XLMRobertaConfig\n",
    "model_class = XLMRobertaClassifier\n",
    "tokenizer_class = XLMRobertaTokenizer\n",
    "\n",
    "config = config_class.from_pretrained(MODEL_NAME, num_labels=num_labels) \n",
    "\n",
    "model = model_class.from_pretrained(MODEL_NAME, config=config)\n",
    "# print('Model=\\n',model,'\\n')\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(MODEL_NAME, do_lower_case=False)\n",
    "# print('Tokenizer=',tokenizer,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:00<00:00, 4555.39it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4766.35it/s]\n"
     ]
    }
   ],
   "source": [
    "# build the pytorch dataloader\n",
    "train_dataset = RobertaDataset('output/train_3_original.pkl', tokenizer)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "dev_dataset = RobertaDataset('output/dev_1_original.pkl', tokenizer)\n",
    "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = len(train_loader) // gradient_accumulation_steps * num_train_epochs\n",
    "optimizer_grouped_parameters = []\n",
    "custom_parameter_names = set()\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters.extend(\n",
    "    [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if n not in custom_parameter_names and not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if n not in custom_parameter_names and any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "warmup_steps = math.ceil(t_total * warmup_ratio)\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(preds, model_outputs, labels, eval_examples=None, multi_label=False):\n",
    "    assert len(preds) == len(labels)\n",
    "    mismatched = labels != preds\n",
    "    wrong = [i for (i, v) in zip(eval_examples, mismatched) if v.any()]\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds, labels=[0, 1]).ravel()\n",
    "    scores = np.array([softmax(element)[1] for element in model_outputs])\n",
    "    fpr, tpr, thresholds = roc_curve(labels, scores)\n",
    "    auroc = auc(fpr, tpr)\n",
    "    auprc = average_precision_score(labels, scores)\n",
    "    return (\n",
    "        {\n",
    "            **{\"mcc\": mcc, \"tp\": tp, \"tn\": tn, \"fp\": fp, \"fn\": fn, \"auroc\": auroc, \"auprc\": auprc},\n",
    "        },\n",
    "        wrong,\n",
    "    )\n",
    "\n",
    "def print_confusion_matrix(result):\n",
    "    print('confusion matrix:')\n",
    "    print('            predicted    ')\n",
    "    print('          0     |     1')\n",
    "    print('    ----------------------')\n",
    "    print('   0 | ',format(result['tn'],'5d'),' | ',format(result['fp'],'5d'))\n",
    "    print('gt -----------------------')\n",
    "    print('   1 | ',format(result['fn'],'5d'),' | ',format(result['tp'],'5d'))\n",
    "    print('---------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(output, targets):\n",
    "    # get precision and recall for each class\n",
    "    precision = [0] * num_labels\n",
    "    recall = [0] * num_labels\n",
    "    for i in range(num_labels):\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        for j in range(len(output)):\n",
    "            if output[j] == i:\n",
    "                if targets[j] == i:\n",
    "                    tp += 1\n",
    "                else:\n",
    "                    fp += 1\n",
    "            elif targets[j] == i:\n",
    "                fn += 1\n",
    "        if tp == 0:\n",
    "            precision[i] = 0\n",
    "            recall[i] = 0\n",
    "        else:\n",
    "            precision[i] = tp / (tp + fp)\n",
    "            recall[i] = tp / (tp + fn)\n",
    "    # calculate f1 score for each class\n",
    "    f1 = [0] * num_labels\n",
    "    for i in range(num_labels):\n",
    "        if precision[i] == 0 and recall[i] == 0:\n",
    "            f1[i] = 0\n",
    "        else:\n",
    "            f1[i] = 2 * precision[i] * recall[i] / (precision[i] + recall[i])\n",
    "    # print precision, recall, f1 score for each class\n",
    "    print('class 0: ')\n",
    "    print('precision: ', precision)\n",
    "    print('recall: ', recall)\n",
    "    print('f1 score: ', f1)\n",
    "    # calculate weighted average f1 score\n",
    "    f1_weighted = 0\n",
    "    for i in range(num_labels):\n",
    "        f1_weighted += f1[i] * (sum(targets == i) / len(targets))\n",
    "    # print weighted average f1 score\n",
    "    print('weighted average f1 score: ', f1_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "model.zero_grad()\n",
    "\n",
    "for epoch in range(num_train_epochs):\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    \n",
    "    for batch in tqdm(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['targets'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        model.zero_grad()\n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "    #evaluate model with test_df at the end of the epoch.\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    n_batches = len(dev_loader)\n",
    "    preds = np.empty((len(dev_dataset), num_labels))\n",
    "    out_label_ids = np.empty((len(dev_dataset)))\n",
    "    model.eval()\n",
    "    \n",
    "    for i,test_batch in enumerate(dev_loader):\n",
    "        with torch.no_grad():\n",
    "            input_ids = test_batch['input_ids'].to(device)\n",
    "            attention_mask = test_batch['attention_mask'].to(device)\n",
    "            labels = test_batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "            eval_loss += tmp_eval_loss.item()\n",
    "            \n",
    "        nb_eval_steps += 1\n",
    "        start_index = test_batch_size * i\n",
    "        end_index = start_index + test_batch_size if i != (n_batches - 1) else len(dev_dataset)\n",
    "        preds[start_index:end_index] = logits.detach().cpu().numpy()\n",
    "        out_label_ids[start_index:end_index] = test_batch[\"labels\"].detach().cpu().numpy()\n",
    "        \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    model_outputs = preds\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    score = f1_score(preds, )    \n",
    "    print('epoch',epoch,'Training avg loss',np.mean(epoch_loss))\n",
    "    print('epoch',epoch,'Testing  avg loss',eval_loss)\n",
    "    print(result)\n",
    "    # print f1 score\n",
    "    print('f1 score:',f1_score(out_label_ids, preds))\n",
    "    print('---------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (default, Sep  4 2020, 07:30:14) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0eefc6a63c6d719296cee5685f23fefb92aa63e2e9fdaf52ddbdc4ce266c7bb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
