{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing out the roberta model\n",
    "import torch\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "MODEL_NAME = 'xlm-roberta-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset_path, tokenizer, category=False):\n",
    "        dataset = pd.read_pickle(dataset_path)\n",
    "        self.data = dataset['tokens'].apply(lambda x: ' '.join(x))\n",
    "        max_len = 0\n",
    "        for i in tqdm(range(len(self.data))):\n",
    "            input_ids = tokenizer.encode(self.data[i], add_special_tokens=True)\n",
    "            max_len = max(max_len, len(input_ids))\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        if category:\n",
    "            self.targets = dataset['category']\n",
    "        else:\n",
    "            self.targets = dataset['stance']\n",
    "        del dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        data = self.data[item]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            data,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'targets': torch.tensor(self.targets[item], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6988/6988 [00:01<00:00, 4592.95it/s]\n"
     ]
    }
   ],
   "source": [
    "# build the pytorch dataloader\n",
    "train_dataset = RobertaDataset('output/train_1_original.pkl', XLMRobertaTokenizer.from_pretrained(MODEL_NAME))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertPreTrainedModel, XLMRobertaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = XLMRobertaConfig.from_pretrained(MODEL_NAME, num_labels=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "\n",
    "class RobertaModel(torch.nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super(RobertaModel, self).__init__()\n",
    "        self.model = XLMRobertaModel.from_pretrained(model_name)\n",
    "        self.drop = torch.nn.Dropout(p=0.3)\n",
    "        self.out = torch.nn.Linear(self.model.config.hidden_size, 3)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        output = self.drop(output)\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = \n",
    "model = XLMRobertaModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_pickle('output/train_1_original.pkl')\n",
    "dev_data = pd.read_pickle('output/dev_1_original.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xlm_roberta_encode(texts, tokenizer, max_length=512,):\n",
    "    ct = len(texts)\n",
    "    input_ids = np.ones((ct, max_length), dtype='int32')\n",
    "    attention_mask = np.zeros((ct, max_length), dtype='int32')\n",
    "    token_type_ids = np.zeros((ct, max_length), dtype='int32') # Not used in text classification\n",
    "\n",
    "    for k, text in enumerate(texts):\n",
    "        # Tokenize\n",
    "        tok_text = tokenizer.tokenize(text)\n",
    "        \n",
    "        # Truncate and convert tokens to numerical IDs\n",
    "        enc_text = tokenizer.convert_tokens_to_ids(tok_text[:(max_length-2)])\n",
    "        \n",
    "        input_length = len(enc_text) + 2\n",
    "        input_length = input_length if input_length < max_length else max_length\n",
    "        \n",
    "        # Add tokens [CLS] and [SEP] at the beginning and the end\n",
    "        input_ids[k,:input_length] = np.asarray([0] + enc_text + [2], dtype='int32')\n",
    "        \n",
    "        # Set to 1s in the attention input\n",
    "        attention_mask[k,:input_length] = 1\n",
    "\n",
    "    return {\n",
    "        'input_word_ids': input_ids,\n",
    "        'input_mask': attention_mask,\n",
    "        'input_type_ids': token_type_ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = xlm_roberta_encode(train_data['text'], tokenizer, max_length=max_len)\n",
    "x_dev = xlm_roberta_encode(dev_data['text'], tokenizer, max_length=max_len)\n",
    "\n",
    "# convert to tensors\n",
    "x_train = {k: torch.tensor(v) for k, v in x_train.items()}\n",
    "x_dev = {k: torch.tensor(v) for k, v in x_dev.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data['stance'].to_numpy(dtype='int32')\n",
    "y_dev = dev_data['stance'].to_numpy(dtype='int32')\n",
    "\n",
    "# convert to tensors\n",
    "\n",
    "y_train = torch.tensor(y_train)\n",
    "y_dev = torch.tensor(y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_categories):\n",
    "    input_word_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_word_ids')\n",
    "    input_mask = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_mask')\n",
    "    input_type_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_type_ids')\n",
    "\n",
    "    # Import RoBERTa model from HuggingFace\n",
    "    x = model(input_word_ids, attention_mask=input_mask, token_type_ids=input_type_ids)\n",
    "    # Huggingface transformers have multiple outputs, embeddings are the first one,\n",
    "    # so let's slice out the first position\n",
    "    x = x[0]\n",
    "\n",
    "    x = tf.keras.layers.Dropout(0.1)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(n_categories, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=x)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(lr=1e-5),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (default, Sep  4 2020, 07:30:14) \n[GCC 7.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0eefc6a63c6d719296cee5685f23fefb92aa63e2e9fdaf52ddbdc4ce266c7bb3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
