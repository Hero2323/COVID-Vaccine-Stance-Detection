{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from tqdm import tqdm\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 4,
>>>>>>> 0685b38a5a85cf2c4437e8fe15bd4933aebf68e7
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "train counts\n",
      " 2    5538\n",
      "1    1012\n",
      "0     438\n",
      "Name: stance, dtype: int64\n",
      "dev counts\n",
      " 2    804\n",
      "1    126\n",
      "0     70\n",
      "Name: stance, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Read the datasets\n",
    "train = pd.read_csv('dataset/train.csv')\n",
    "train['stance'] += 1\n",
    "print('train counts\\n', train['stance'].value_counts())\n",
    "dev = pd.read_csv('dataset/dev.csv')\n",
    "dev['stance'] += 1\n",
    "dev['stance'].value_counts()\n",
    "print('dev counts\\n', dev['stance'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
=======
      "Collecting transformers\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\swak\\anaconda3\\lib\\site-packages (from transformers) (1.20.1)\n",
      "Requirement already satisfied: requests in c:\\users\\swak\\anaconda3\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\swak\\anaconda3\\lib\\site-packages (from transformers) (4.59.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\swak\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\swak\\anaconda3\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\swak\\anaconda3\\lib\\site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\swak\\anaconda3\\lib\\site-packages (from transformers) (20.9)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Using cached huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\swak\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\swak\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\swak\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\swak\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\swak\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\swak\\anaconda3\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
>>>>>>> 0685b38a5a85cf2c4437e8fe15bd4933aebf68e7
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the train dataset, there are 4 different versions made from it:\n",
    "    # 1. The original dataset with original counts\n",
    "    # 2. The dataset but with 500 tweets per class\n",
    "    # 3. The dataset but with 1000 tweets per class\n",
    "    # 4. The dataset but up 2500 tweets per class\n",
    "\n",
    "train_1 = train.copy()\n",
    "dev_1 = dev.copy()\n",
    "\n",
    "tmp1 = train[train['stance'] == 0].sample(500, random_state=42, replace=True)\n",
    "tmp2 = train[train['stance'] == 1].sample(500, random_state=42, replace=True)\n",
    "tmp3 = train[train['stance'] == 2].sample(500, random_state=42, replace=True)\n",
    "\n",
    "train_2 = pd.concat([tmp1, tmp2, tmp3])\n",
    "\n",
    "tmp1 = train[train['stance'] == 0].sample(1000, random_state=42, replace=True)\n",
    "tmp2 = train[train['stance'] == 1].sample(1000, random_state=42, replace=True)\n",
    "tmp3 = train[train['stance'] == 2].sample(1000, random_state=42, replace=True)\n",
    "\n",
    "train_3 = pd.concat([tmp1, tmp2, tmp3])\n",
    "\n",
    "tmp1 = train[train['stance'] == 0].sample(2500, random_state=42, replace=True)\n",
    "tmp2 = train[train['stance'] == 1].sample(2500, random_state=42, replace=True)\n",
    "tmp3 = train[train['stance'] == 2].sample(2500, random_state=42, replace=True)\n",
    "\n",
    "train_4 = pd.concat([tmp1, tmp2, tmp3])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
=======
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    5538\n",
       "1    1012\n",
       "0     438\n",
       "Name: stance, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> 0685b38a5a85cf2c4437e8fe15bd4933aebf68e7
    }
   ],
   "source": [
    "# Some pytorch preparations\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('device:', device)"
   ]
  },
  {
<<<<<<< HEAD
   "attachments": {},
   "cell_type": "markdown",
=======
   "cell_type": "code",
   "execution_count": 4,
>>>>>>> 0685b38a5a85cf2c4437e8fe15bd4933aebf68e7
   "metadata": {},
   "source": [
    "# Using Arabert (No stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "[2022-12-14 19:27:36,119 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv2 were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
=======
      "[2022-12-13 16:52:02,926 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "Downloading: 100%|██████████| 543M/543M [02:36<00:00, 3.48MB/s] \n",
      "c:\\Users\\swak\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\swak\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv2 were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 7500/7500 [10:51<00:00, 11.51it/s]\n"
>>>>>>> 0685b38a5a85cf2c4437e8fe15bd4933aebf68e7
     ]
    }
   ],
   "source": [
    "# The first preprocessing pipeline, using the arabert model\n",
    "# This pipeline produces 2 outputs for every dataset:\n",
    "    # 1. tokenized data for every tweet --> This is so this data can be used by others for feature extraction\n",
    "    # 2. word embeddings for every tweet --> This can directly be used by the model\n",
    "\n",
    "model_name = \"aubmindlab/bert-base-arabertv2\"\n",
    "arabert_prep = ArabertPreprocessor(model_name=model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "bert_model = BertModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "def preprocess_arabert(text, embedding=True):\n",
    "    \"\"\"\n",
    "    This function preprocesses the text using arabert.\n",
    "    It's essentially a full pipeline that even returns the word embeddings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str\n",
    "        The text to be preprocessed\n",
    "    embedding: bool\n",
    "        Whether to return the word embeddings or not\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    output: list\n",
    "        The preprocessed text\n",
    "    \"\"\"\n",
    "\n",
    "    def clean_text(text):\n",
    "        # remove any word with + in it\n",
    "        text = re.sub(r'\\S*\\+\\S*', '', text)\n",
    "        # remove non arabic characters\n",
    "        text = re.sub(r'[^\\u0600-\\u06FF]', ' ', text)\n",
    "        # remove extra spaces\n",
    "        return text\n",
    "\n",
    "    output = arabert_prep.preprocess(text)\n",
    "    output = clean_text(output)\n",
    "    tokenized = tokenizer.tokenize(output)\n",
    "\n",
    "    if embedding:\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "        tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(tokens_tensor)\n",
    "            encoded_layers = outputs[0]\n",
    "            encoded_layers = encoded_layers.view(-1, 768)\n",
    "        del tokens_tensor\n",
    "        del outputs\n",
    "        return encoded_layers\n",
    "    else:\n",
    "        return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6988/6988 [00:18<00:00, 383.27it/s]\n",
      "100%|██████████| 6988/6988 [01:31<00:00, 76.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# Use the first preprocessing pipeline to preprocess the datasets\n",
    "tqdm.pandas()\n",
    "\n",
    "train_1['tokenized'] = train_1['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=False))\n",
    "train_1['embeddings'] = train_1['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=True))\n",
    "\n",
    "# Write the datasets to pickle files -- So that it can be used by any other files easily\n",
    "train_1.to_pickle('output/train_1_arabert.pkl')\n",
    "\n",
    "# Can later be read using the following code:\n",
    "    # train_1 = pd.read_pickle('output/train_1.pkl')\n",
    "# The dimensions for every word is (1, 768)\n",
    "# This means that every sentence will have a dimension of (sentence_length, 768)\n",
    "\n",
    "# clear the memory\n",
    "del train_1"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [00:03<00:00, 386.53it/s]\n",
      "100%|██████████| 1500/1500 [00:19<00:00, 75.78it/s]\n"
     ]
    }
   ],
   "source": [
    "train_2['tokenized'] = train_2['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=False))\n",
    "train_2['embeddings'] = train_2['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=True))\n",
    "train_2.to_pickle('output/train_2_arabert.pkl')\n",
    "del train_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:07<00:00, 380.97it/s]\n",
      "100%|██████████| 3000/3000 [00:39<00:00, 75.74it/s]\n"
     ]
    }
   ],
   "source": [
    "train_3['tokenized'] = train_3['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=False))\n",
    "train_3['embeddings'] = train_3['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=True))\n",
    "train_3.to_pickle('output/train_3_arabert.pkl')\n",
    "del train_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7500/7500 [00:19<00:00, 387.36it/s]\n",
      "100%|██████████| 7500/7500 [01:37<00:00, 76.64it/s]\n"
     ]
    }
   ],
   "source": [
    "train_4['tokenized'] = train_4['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=False))\n",
    "train_4['embeddings'] = train_4['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=True))\n",
    "train_4.to_pickle('output/train_4_arabert.pkl')\n",
    "del train_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 404.44it/s]\n",
      "100%|██████████| 1000/1000 [00:12<00:00, 77.85it/s]\n"
     ]
    }
   ],
   "source": [
    "dev_1['tokenized'] = dev_1['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=False))\n",
    "dev_1['embeddings'] = dev_1['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=True))\n",
    "dev_1.to_pickle('output/dev_1_arabert.pkl')\n",
    "del dev_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
=======
   "execution_count": 6,
>>>>>>> 0685b38a5a85cf2c4437e8fe15bd4933aebf68e7
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the bert_model and the arabert_prep & tokenizer\n",
    "del bert_model\n",
    "del arabert_prep\n",
    "del tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using our function (uses stopwords)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
=======
   "execution_count": 7,
>>>>>>> 0685b38a5a85cf2c4437e8fe15bd4933aebf68e7
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is made because the process of cleaning arabic text is complex\n",
    "# and depends on many unicodes done in many steps\n",
    "def clean_arabic(text):\n",
    "    # ! to understand this nonsense you need this link open \n",
    "    \"\"\" https://ar.wikipedia.org/wiki/%D8%A7%D9%84%D8%AE%D8%B7_\n",
    "        %D8%A7%D9%84%D8%B9%D8%B1%D8%A8%D9%8A_%D9%81%D9%8A_%D9%8A%D9\n",
    "        %88%D9%86%D9%8A%D9%83%D9%88%D8%AF   \"\"\"\n",
    "\n",
    "    # remove التشكيل\n",
    "    text = re.sub(r'[\\u0600-\\u061F]', '', text)\n",
    "    text = re.sub(r'[\\u064B-\\u066D]', '', text)\n",
    "\n",
    "    # Because of all the idiots that were bypassing twitters' spam filters using\n",
    "    # special characters like this idiot: كو.ر.ونا We'll remove all the special \n",
    "    # before everything else\n",
    "\n",
    "    # remove special characters\n",
    "    text = re.sub(r'[\\u0024-\\u003F]', '', text)\n",
    "    text = re.sub(r'[\\u005B-\\u0060]', '', text)\n",
    "    text = re.sub(r'[\\u007B-\\u007E]', '', text)\n",
    "\n",
    "    # replace weird characters with more standard ones\n",
    "    # 1. replace چ with ج\n",
    "    text = re.sub(r'چ','ج',text)\n",
    "\n",
    "    # 2. replace ڤ ڨ with ف\n",
    "    text = re.sub(r'ڤ','ف',text)\n",
    "    text = re.sub(r'ڨ','ف',text)\n",
    "\n",
    "    # 3. replace ڠ with ق\n",
    "    text = re.sub(r'ڠ','غ',text)\n",
    "    \n",
    "    # 4. replace ٱ\tٲ\tٳ\t◌ٴ\tٵ with ا\n",
    "    string = ['ٱ','ٲ','ٳ','ٴ','ٵ', 'آ', 'أ', 'إ']\n",
    "    for char in string:\n",
    "        text = re.sub(char,'ا',text)\n",
    "\n",
    "    # 5. replace ٶ\tٷ with و\n",
    "    string = ['ٶ','ٷ']\n",
    "    for char in string:\n",
    "        text = re.sub(char,'و',text)\n",
    "\n",
    "    # 6. replace ٸ ی with ي\n",
    "    text = re.sub(r'ٸ','ي',text) \n",
    "    text = re.sub(r'ی','ي',text)\n",
    "    \n",
    "    # 7. replace پ\twith ب\n",
    "    text = re.sub(r'پ','ب',text)\n",
    "\n",
    "    # 8. replace ژ with ز\n",
    "    text = re.sub(r'ژ','ز',text)\n",
    "\n",
    "    # 9. replace ک ڪ ګ ڬ ڭ ڮ گ ڰ ڱ ڲ ڳ ڴ with ك\n",
    "    string = ['ک', 'ڪ', 'ګ', 'ڬ', 'ڭ', 'ڮ', 'گ', 'ڰ', 'ڱ', 'ڲ', 'ڳ', 'ڴ']\n",
    "    for char in string:\n",
    "        text = re.sub(char,'ك',text)\n",
    "    # 10. replace ھ with ه\n",
    "    text = re.sub(r'ھ','ه',text)\n",
    "\n",
    "    # remove all extra arabic characters (shift + ت) \n",
    "    text = re.sub(r'ـ','',text)\n",
    "\n",
    "    # remove non arabic characters\n",
    "    text = re.sub(r'[^\\u0620-\\u064A\\s]',' ',text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 14,
=======
   "execution_count": 8,
>>>>>>> 0685b38a5a85cf2c4437e8fe15bd4933aebf68e7
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\" removes all non arabic characters & replaces all spaces with a single space \"\"\"\n",
    "    \n",
    "    # remove all words with # in them\n",
    "    text = re.sub(r'[^\\s]*#[^\\s]*',' ',text)\n",
    "    \n",
    "    # arabic letters clean up \n",
    "    text = clean_arabic(text)\n",
    "        \n",
    "    # replace all white spaces with a single space\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    \n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 22,
=======
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train_data = train['text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
>>>>>>> 0685b38a5a85cf2c4437e8fe15bd4933aebf68e7
   "metadata": {},
   "outputs": [],
   "source": [
    "def original_preprocess(dataset):\n",
    "    tqdm.pandas()\n",
    "    dataset['cleaned_data'] = dataset['text'].progress_apply(lambda x: clean_text(x))\n",
    "    dataset['tokenized_data'] = dataset['cleaned_data'].progress_apply(lambda x: nltk.word_tokenize(x))\n",
    "    dataset.drop(['cleaned_data'], axis=1, inplace=True)\n",
    "\n",
<<<<<<< HEAD
    "    # remove stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words('arabic')\n",
    "\n",
    "    # Here we're looking for more stopwords that are 2 characters or less\n",
    "    # we spend hours doing just this for two or three character words\n",
    "    stopwords += ['ال', 'اي', 'ان', 'تم', 'بن', \n",
    "                'او', 'اي', 'عم', 'ام', 'رض',\n",
    "                'في', 'فى', 'رب', 'سم', 'خط',\n",
    "                'ول', 'زي', 'دي', 'اذ', 'ده',\n",
    "                'دى', 'انه', 'ابو', 'احد']\n",
    "    dataset['tokens_no_stopwords'] = dataset['tokenized_data'].progress_apply(lambda x: [word for word in x if word not in stopwords]) \n",
    "    dataset.drop(['tokenized_data'], axis=1, inplace=True)\n",
    "    dataset['tokens'] = dataset['tokens_no_stopwords'].progress_apply(lambda x: [ISRIStemmer().stem(word) for word in x])\n",
    "    dataset.drop(['tokens_no_stopwords'], axis=1, inplace=True)\n",
    "\n",
    "    return dataset"
=======
    "# currently this line just tokenizes the text using a space (I think)\n",
    "tokenized_train_data = [nltk.word_tokenize(text) for text in cleaned_train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3185    الموت أرحم عندي من انو  آخد لقاح موهوب من دولة...\n",
       "3185    الموت أرحم عندي من انو  آخد لقاح موهوب من دولة...\n",
       "3185    الموت أرحم عندي من انو  آخد لقاح موهوب من دولة...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text'][3185]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('arabic')\n",
    "tokenized_no_stopwords_train_data = [[word for word in text if word not in stopwords] for text in tokenized_train_data]\n",
    "\n",
    "# Here we're looking for more stopwords that are 2 characters or less\n",
    "# we spend hours doing just this for two or three character words\n",
    "stopwords_extracted_from_dataset = ['ال', 'اي', 'ان', 'تم', 'بن', \n",
    "                                    'او', 'اي', 'عم', 'ام', 'رض',\n",
    "                                    'في', 'فى', 'رب', 'سم', 'خط',\n",
    "                                    'ول', 'زي', 'دي', 'اذ', 'ده',\n",
    "                                    'دى', 'انه', 'ابو', 'احد']\n",
    "\n",
    "tokenized_no_stopwords_train_data_v2 = [[word for word in text if word not in stopwords_extracted_from_dataset] for text in tokenized_no_stopwords_train_data]"
>>>>>>> 0685b38a5a85cf2c4437e8fe15bd4933aebf68e7
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 20,
=======
   "execution_count": 13,
>>>>>>> 0685b38a5a85cf2c4437e8fe15bd4933aebf68e7
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the train dataset, there are 4 different versions made from it:\n",
    "    # 1. The original dataset with original counts\n",
    "    # 2. The dataset but with 500 tweets per class\n",
    "    # 3. The dataset but with 1000 tweets per class\n",
    "    # 4. The dataset but up 2500 tweets per class\n",
    "\n",
<<<<<<< HEAD
    "train_1 = train.copy()\n",
    "dev_1 = dev.copy()\n",
    "\n",
    "tmp1 = train[train['stance'] == 0].sample(500, random_state=42, replace=True)\n",
    "tmp2 = train[train['stance'] == 1].sample(500, random_state=42, replace=True)\n",
    "tmp3 = train[train['stance'] == 2].sample(500, random_state=42, replace=True)\n",
=======
    "stopwords_set.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-e790e6043162>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mstemmed_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stemmed_train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\swak\\anaconda3\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_SparseArray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"module 'pandas' has no attribute '{name}'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "stemmed_train = []\n",
    "\n",
    "pd.to_csv('stemmed_train.csv')\n",
    "\n",
    "\n",
    "for text in tqdm(tokenized_no_stopwords_train_data_v2):\n",
    "    curr_list = []\n",
    "    for word in text:\n",
    "        curr_list.append(ISRIStemmer().stem(word))\n",
    "    stemmed_train.append(curr_list)\n",
>>>>>>> 0685b38a5a85cf2c4437e8fe15bd4933aebf68e7
    "\n",
    "train_2 = pd.concat([tmp1, tmp2, tmp3]).sort_index()\n",
    "\n",
    "tmp1 = train[train['stance'] == 0].sample(1000, random_state=42, replace=True)\n",
    "tmp2 = train[train['stance'] == 1].sample(1000, random_state=42, replace=True)\n",
    "tmp3 = train[train['stance'] == 2].sample(1000, random_state=42, replace=True)\n",
    "\n",
    "train_3 = pd.concat([tmp1, tmp2, tmp3]).sort_index()\n",
    "\n",
    "tmp1 = train[train['stance'] == 0].sample(2500, random_state=42, replace=True)\n",
    "tmp2 = train[train['stance'] == 1].sample(2500, random_state=42, replace=True)\n",
    "tmp3 = train[train['stance'] == 2].sample(2500, random_state=42, replace=True)\n",
    "\n",
    "train_4 = pd.concat([tmp1, tmp2, tmp3]).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1 = original_preprocess(train_1)\n",
    "train_2 = original_preprocess(train_2)\n",
    "train_3 = original_preprocess(train_3)\n",
    "train_4 = original_preprocess(train_4)\n",
    "dev_1 = original_preprocess(dev_1)\n",
    "\n",
    "# save the datasets to pickle files\n",
    "train_1.to_pickle('output/train_1_original.pkl')\n",
    "train_2.to_pickle('output/train_2_original.pkl')\n",
    "train_3.to_pickle('output/train_3_original.pkl')\n",
    "train_4.to_pickle('output/train_4_original.pkl')\n",
    "dev_1.to_pickle('output/dev_1_original.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "655c12636761ad29ab72f84a68a8a53faa13a1d27df70400707a6bbc70c23f25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
