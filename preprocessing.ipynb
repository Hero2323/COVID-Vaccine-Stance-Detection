{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import nltk\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train stance counts\n",
      " 2    5538\n",
      "1    1012\n",
      "0     438\n",
      "Name: stance, dtype: int64\n",
      "dev stance counts\n",
      " 2    804\n",
      "1    126\n",
      "0     70\n",
      "Name: stance, dtype: int64\n",
      "---------------------\n",
      "train category counts\n",
      " info_news       3616\n",
      "personal        1025\n",
      "celebrity        975\n",
      "plan             606\n",
      "unrelated        323\n",
      "others           167\n",
      "requests         112\n",
      "rumors            79\n",
      "advice            67\n",
      "restrictions      18\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Read the datasets\n",
    "train = pd.read_csv('dataset/train.csv')\n",
    "train['stance'] += 1\n",
    "print('train stance counts\\n', train['stance'].value_counts())\n",
    "dev = pd.read_csv('dataset/dev.csv')\n",
    "dev['stance'] += 1\n",
    "dev['stance'].value_counts()\n",
    "print('dev stance counts\\n', dev['stance'].value_counts())\n",
    "print('---------------------')\n",
    "print('train category counts\\n', train['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the train dataset, there are 4 different versions made from it:\n",
    "    # 1. The original dataset with original counts\n",
    "    # 2. The dataset but with 500 tweets per class\n",
    "    # 3. The dataset but with 1000 tweets per class\n",
    "    # 4. The dataset but up 2500 tweets per class\n",
    "\n",
    "def equalize_datset_stance(dataset, count):\n",
    "    copy = dataset.copy()\n",
    "    def equalize_stance(df, stance, count):\n",
    "        if df[df['stance'] == stance].shape[0] > count:\n",
    "            tmp = df[df['stance'] == stance].sample(count, random_state=42)\n",
    "        else:\n",
    "            tmp = df[df['stance'] == stance]\n",
    "            tmp = tmp.append(df[df['stance'] == stance].sample(count - df[df['stance'] == stance].shape[0], random_state=42, replace=True))\n",
    "        return tmp\n",
    "    temp0 = equalize_stance(copy, 0, count)\n",
    "    temp1 = equalize_stance(copy, 1, count)\n",
    "    temp2 = equalize_stance(copy, 2, count)\n",
    "    return pd.concat([temp0, temp1, temp2])\n",
    "\n",
    "train_1 = train.copy()\n",
    "dev_1 = dev.copy()\n",
    "\n",
    "train_2 = equalize_datset_stance(train, 500)\n",
    "train_3 = equalize_datset_stance(train, 1000)\n",
    "train_4 = equalize_datset_stance(train, 2500)\n",
    "\n",
    "def equalize_dataset_category(dataset, count):\n",
    "    def equalize_category(df, category, count):\n",
    "        if df[df['category'] == category].shape[0] > count:\n",
    "            tmp = df[df['category'] == category].sample(count, random_state=42)\n",
    "        else:\n",
    "            tmp = df[df['category'] == category]\n",
    "            tmp = tmp.append(df[df['category'] == category].sample(count - df[df['category'] == category].shape[0], random_state=42, replace=True))\n",
    "        return tmp\n",
    "    tmp1 = equalize_category(dataset, 'info_news', count)\n",
    "    tmp2 = equalize_category(dataset, 'personal', count)\n",
    "    tmp3 = equalize_category(dataset, 'celebrity', count)\n",
    "    tmp4 = equalize_category(dataset, 'plan', count)\n",
    "    tmp5 = equalize_category(dataset, 'unrelated', count)\n",
    "    tmp6 = equalize_category(dataset, 'others', count)\n",
    "    tmp7 = equalize_category(dataset, 'requests', count)\n",
    "    tmp8 = equalize_category(dataset, 'rumors', count)\n",
    "    tmp9 = equalize_category(dataset, 'advice', count)\n",
    "    tmp10 = equalize_category(dataset, 'restrictions', count)\n",
    "    return pd.concat([tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7, tmp8, tmp9, tmp10])\n",
    "\n",
    "# sample the train dataset such that every category contains 50 tweets\n",
    "train_5 = equalize_dataset_category(train, 50)\n",
    "\n",
    "# sample the train dataset such that every category contains 100 tweets\n",
    "train_6 = equalize_dataset_category(train, 100)\n",
    "\n",
    "# sample the train dataset such that every category contains 150 tweets\n",
    "train_7 = equalize_dataset_category(train, 150)\n",
    "\n",
    "# sample the train dataset such that every category contains 250 tweets\n",
    "train_8 = equalize_dataset_category(train, 250)\n",
    "\n",
    "# sample the train dataset such that every category contains 500 tweets\n",
    "\n",
    "train_9 = equalize_dataset_category(train, 500)\n",
    "\n",
    "# sample the train dataset such that every category contains 750 tweets\n",
    "train_10 = equalize_dataset_category(train, 750)\n",
    "\n",
    "# sample the train dataset such that every category contains 1000 tweets\n",
    "train_11 = equalize_dataset_category(train, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Some pytorch preparations\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('device:', device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Arabert (No stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-12-26 12:31:24,207 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv2 were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13998/1678910831.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0marabert_prep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArabertPreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_arabert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "# The first preprocessing pipeline, using the arabert model\n",
    "# This pipeline produces 2 outputs for every dataset:\n",
    "    # 1. tokenized data for every tweet --> This is so this data can be used by others for feature extraction\n",
    "    # 2. word embeddings for every tweet --> This can directly be used by the model\n",
    "\n",
    "model_name = \"aubmindlab/bert-base-arabertv2\"\n",
    "arabert_prep = ArabertPreprocessor(model_name=model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "bert_model = BertModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "def preprocess_arabert(text, embedding=True):\n",
    "    \"\"\"\n",
    "    This function preprocesses the text using arabert.\n",
    "    It's essentially a full pipeline that even returns the word embeddings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str\n",
    "        The text to be preprocessed\n",
    "    embedding: bool\n",
    "        Whether to return the word embeddings or not\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    output: list\n",
    "        The preprocessed text\n",
    "    \"\"\"\n",
    "\n",
    "    def clean_text(text):\n",
    "        # remove any word with + in it\n",
    "        text = re.sub(r'\\S*\\+\\S*', '', text)\n",
    "        # remove non arabic characters\n",
    "        text = re.sub(r'[^\\u0600-\\u06FF]', ' ', text)\n",
    "        # remove extra spaces\n",
    "        return text\n",
    "\n",
    "    output = arabert_prep.preprocess(text)\n",
    "    output = clean_text(output)\n",
    "    tokenized = tokenizer.tokenize(output)\n",
    "\n",
    "    if embedding:\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "        tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(tokens_tensor)\n",
    "            encoded_layers = outputs[0]\n",
    "            encoded_layers = encoded_layers.view(-1, 768)\n",
    "        del tokens_tensor\n",
    "        del outputs\n",
    "        return encoded_layers\n",
    "    else:\n",
    "        return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6988/6988 [00:18<00:00, 370.60it/s]\n",
      "100%|██████████| 6988/6988 [01:34<00:00, 74.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# Use the first preprocessing pipeline to preprocess the datasets\n",
    "tqdm.pandas()\n",
    "\n",
    "train_1['tokenized'] = train_1['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=False))\n",
    "train_1['embeddings'] = train_1['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=True))\n",
    "\n",
    "# Write the datasets to pickle files -- So that it can be used by any other files easily\n",
    "train_1.to_pickle('output/train_1_arabert.pkl')\n",
    "\n",
    "# Can later be read using the following code:\n",
    "    # train_1 = pd.read_pickle('output/train_1.pkl')\n",
    "# The dimensions for every word is (1, 768)\n",
    "# This means that every sentence will have a dimension of (sentence_length, 768)\n",
    "\n",
    "# clear the memory\n",
    "del train_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [00:04<00:00, 337.43it/s]\n",
      "100%|██████████| 1500/1500 [00:19<00:00, 77.14it/s]\n"
     ]
    }
   ],
   "source": [
    "train_2['tokenized'] = train_2['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=False))\n",
    "train_2['embeddings'] = train_2['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=True))\n",
    "train_2.to_pickle('output/train_2_arabert.pkl')\n",
    "del train_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:07<00:00, 384.80it/s]\n",
      "100%|██████████| 3000/3000 [00:38<00:00, 78.08it/s]\n"
     ]
    }
   ],
   "source": [
    "train_3['tokenized'] = train_3['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=False))\n",
    "train_3['embeddings'] = train_3['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=True))\n",
    "train_3.to_pickle('output/train_3_arabert.pkl')\n",
    "del train_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7500/7500 [00:19<00:00, 379.70it/s]\n",
      "100%|██████████| 7500/7500 [01:39<00:00, 75.42it/s]\n"
     ]
    }
   ],
   "source": [
    "train_4['tokenized'] = train_4['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=False))\n",
    "train_4['embeddings'] = train_4['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=True))\n",
    "train_4.to_pickle('output/train_4_arabert.pkl')\n",
    "del train_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:01<00:00, 379.15it/s]\n",
      "100%|██████████| 500/500 [00:06<00:00, 79.65it/s]\n"
     ]
    }
   ],
   "source": [
    "train_5['tokenized'] = train_5['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=False))\n",
    "train_5['embeddings'] = train_5['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=True))\n",
    "train_5.to_pickle('output/train_5_arabert.pkl')\n",
    "del train_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 397.88it/s]\n",
      "100%|██████████| 1000/1000 [00:13<00:00, 74.31it/s]\n"
     ]
    }
   ],
   "source": [
    "train_6['tokenized'] = train_6['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=False))\n",
    "train_6['embeddings'] = train_6['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=True))\n",
    "train_6.to_pickle('output/train_6_arabert.pkl')\n",
    "del train_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [00:03<00:00, 379.78it/s]\n",
      "100%|██████████| 1500/1500 [00:20<00:00, 73.18it/s]\n"
     ]
    }
   ],
   "source": [
    "train_7['tokenized'] = train_7['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=False))\n",
    "train_7['embeddings'] = train_7['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=True))\n",
    "train_7.to_pickle('output/train_7_arabert.pkl')\n",
    "del train_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2500/2500 [00:06<00:00, 377.41it/s]\n",
      "100%|██████████| 2500/2500 [00:34<00:00, 73.40it/s]\n"
     ]
    }
   ],
   "source": [
    "train_8['tokenized'] = train_8['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=False))\n",
    "train_8['embeddings'] = train_8['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=True))\n",
    "train_8.to_pickle('output/train_8_arabert.pkl')\n",
    "del train_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:12<00:00, 398.13it/s]\n",
      "100%|██████████| 5000/5000 [01:04<00:00, 77.83it/s]\n"
     ]
    }
   ],
   "source": [
    "train_9['tokenized'] = train_9['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=False))\n",
    "train_9['embeddings'] = train_9['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=True))\n",
    "train_9.to_pickle('output/train_9_arabert.pkl')\n",
    "del train_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7500/7500 [00:18<00:00, 403.88it/s]\n",
      "100%|██████████| 7500/7500 [01:37<00:00, 76.84it/s]\n"
     ]
    }
   ],
   "source": [
    "train_10['tokenized'] = train_10['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=False))\n",
    "train_10['embeddings'] = train_10['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=True))\n",
    "train_10.to_pickle('output/train_10_arabert.pkl')\n",
    "del train_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:25<00:00, 399.61it/s]\n",
      "100%|██████████| 10000/10000 [02:11<00:00, 75.99it/s]\n"
     ]
    }
   ],
   "source": [
    "train_11['tokenized'] = train_11['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=False))\n",
    "train_11['embeddings'] = train_11['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=True))\n",
    "train_11.to_pickle('output/train_11_arabert.pkl')\n",
    "del train_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:02<00:00, 392.99it/s]\n",
      "100%|██████████| 1000/1000 [00:12<00:00, 77.58it/s]\n"
     ]
    }
   ],
   "source": [
    "dev_1['tokenized'] = dev_1['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=False))\n",
    "dev_1['embeddings'] = dev_1['text'].progress_apply(lambda x: preprocess_arabert(x, embedding=True))\n",
    "dev_1.to_pickle('output/dev_1_arabert.pkl')\n",
    "del dev_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the bert_model and the arabert_prep & tokenizer\n",
    "del bert_model\n",
    "del arabert_prep\n",
    "del tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using our function (uses stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is made because the process of cleaning arabic text is complex\n",
    "# and depends on many unicodes done in many steps\n",
    "def clean_arabic(text):\n",
    "    # ! to understand this nonsense you need this link open \n",
    "    \"\"\" https://ar.wikipedia.org/wiki/%D8%A7%D9%84%D8%AE%D8%B7_\n",
    "        %D8%A7%D9%84%D8%B9%D8%B1%D8%A8%D9%8A_%D9%81%D9%8A_%D9%8A%D9\n",
    "        %88%D9%86%D9%8A%D9%83%D9%88%D8%AF   \"\"\"\n",
    "\n",
    "    # remove التشكيل\n",
    "    text = re.sub(r'[\\u0600-\\u061F]', '', text)\n",
    "    text = re.sub(r'[\\u064B-\\u066D]', '', text)\n",
    "\n",
    "    # Because of all the idiots that were bypassing twitters' spam filters using\n",
    "    # special characters like this idiot: كو.ر.ونا We'll remove all the special \n",
    "    # before everything else\n",
    "\n",
    "    # remove special characters\n",
    "    text = re.sub(r'[\\u0024-\\u003F]', '', text)\n",
    "    text = re.sub(r'[\\u005B-\\u0060]', '', text)\n",
    "    text = re.sub(r'[\\u007B-\\u007E]', '', text)\n",
    "\n",
    "    # replace weird characters with more standard ones\n",
    "    # 1. replace چ with ج\n",
    "    text = re.sub(r'چ','ج',text)\n",
    "\n",
    "    # 2. replace ڤ ڨ with ف\n",
    "    text = re.sub(r'ڤ','ف',text)\n",
    "    text = re.sub(r'ڨ','ف',text)\n",
    "\n",
    "    # 3. replace ڠ with ق\n",
    "    text = re.sub(r'ڠ','غ',text)\n",
    "    \n",
    "    # 4. replace ٱ\tٲ\tٳ\t◌ٴ\tٵ with ا\n",
    "    string = ['ٱ','ٲ','ٳ','ٴ','ٵ', 'آ', 'أ', 'إ']\n",
    "    for char in string:\n",
    "        text = re.sub(char,'ا',text)\n",
    "\n",
    "    # 5. replace ٶ\tٷ with و\n",
    "    string = ['ٶ','ٷ']\n",
    "    for char in string:\n",
    "        text = re.sub(char,'و',text)\n",
    "\n",
    "    # 6. replace ٸ ی with ي\n",
    "    text = re.sub(r'ٸ','ي',text) \n",
    "    text = re.sub(r'ی','ي',text)\n",
    "    \n",
    "    # 7. replace پ\twith ب\n",
    "    text = re.sub(r'پ','ب',text)\n",
    "\n",
    "    # 8. replace ژ with ز\n",
    "    text = re.sub(r'ژ','ز',text)\n",
    "\n",
    "    # 9. replace ک ڪ ګ ڬ ڭ ڮ گ ڰ ڱ ڲ ڳ ڴ with ك\n",
    "    string = ['ک', 'ڪ', 'ګ', 'ڬ', 'ڭ', 'ڮ', 'گ', 'ڰ', 'ڱ', 'ڲ', 'ڳ', 'ڴ']\n",
    "    for char in string:\n",
    "        text = re.sub(char,'ك',text)\n",
    "    # 10. replace ھ with ه\n",
    "    text = re.sub(r'ھ','ه',text)\n",
    "\n",
    "    # remove all extra arabic characters (shift + ت) \n",
    "    text = re.sub(r'ـ','',text)\n",
    "\n",
    "    # remove non arabic characters\n",
    "    text = re.sub(r'[^\\u0620-\\u064A\\s]',' ',text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\" removes all non arabic characters & replaces all spaces with a single space \"\"\"\n",
    "    \n",
    "    # remove all words with # in them\n",
    "    text = re.sub(r'[^\\s]*#[^\\s]*',' ',text)\n",
    "    \n",
    "    # arabic letters clean up \n",
    "    text = clean_arabic(text)\n",
    "        \n",
    "    # replace all white spaces with a single space\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    \n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def original_preprocess(dataset):\n",
    "    tqdm.pandas()\n",
    "    dataset['cleaned_data'] = dataset['text'].progress_apply(lambda x: clean_text(x))\n",
    "    dataset['tokenized_data'] = dataset['cleaned_data'].progress_apply(lambda x: nltk.word_tokenize(x))\n",
    "    dataset.drop(['cleaned_data'], axis=1, inplace=True)\n",
    "\n",
    "    # remove stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words('arabic')\n",
    "\n",
    "    # Here we're looking for more stopwords that are 2 characters or less\n",
    "    # we spend hours doing just this for two or three character words\n",
    "    stopwords += ['ال', 'اي', 'ان', 'تم', 'بن', \n",
    "                'او', 'اي', 'عم', 'ام', 'رض',\n",
    "                'في', 'فى', 'رب', 'سم', 'خط',\n",
    "                'ول', 'زي', 'دي', 'اذ', 'ده',\n",
    "                'دى', 'انه', 'ابو', 'احد']\n",
    "    dataset['tokens_no_stopwords'] = dataset['tokenized_data'].progress_apply(lambda x: [word for word in x if word not in stopwords]) \n",
    "    dataset.drop(['tokenized_data'], axis=1, inplace=True)\n",
    "    dataset['tokens'] = dataset['tokens_no_stopwords'].progress_apply(lambda x: [ISRIStemmer().stem(word) for word in x])\n",
    "    dataset['tokens_no_stem'] = dataset['tokens_no_stopwords']\n",
    "    dataset.drop(['tokens_no_stopwords'], axis=1, inplace=True)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the train dataset, there are 4 different versions made from it:\n",
    "    # 1. The original dataset with original counts\n",
    "    # 2. The dataset but with 500 tweets per class\n",
    "    # 3. The dataset but with 1000 tweets per class\n",
    "    # 4. The dataset but up 2500 tweets per class\n",
    "\n",
    "def equalize_datset_stance(dataset, count):\n",
    "    copy = dataset.copy()\n",
    "    def equalize_stance(df, stance, count):\n",
    "        if df[df['stance'] == stance].shape[0] > count:\n",
    "            tmp = df[df['stance'] == stance].sample(count, random_state=42)\n",
    "        else:\n",
    "            tmp = df[df['stance'] == stance]\n",
    "            tmp = tmp.append(df[df['stance'] == stance].sample(count - df[df['stance'] == stance].shape[0], random_state=42, replace=True))\n",
    "        return tmp\n",
    "    temp0 = equalize_stance(copy, 0, count)\n",
    "    temp1 = equalize_stance(copy, 1, count)\n",
    "    temp2 = equalize_stance(copy, 2, count)\n",
    "    return pd.concat([temp0, temp1, temp2])\n",
    "\n",
    "train_1 = train.copy()\n",
    "dev_1 = dev.copy()\n",
    "\n",
    "train_2 = equalize_datset_stance(train, 500)\n",
    "train_3 = equalize_datset_stance(train, 1000)\n",
    "train_4 = equalize_datset_stance(train, 2500)\n",
    "\n",
    "def equalize_dataset_category(dataset, count):\n",
    "    def equalize_category(df, category, count):\n",
    "        if df[df['category'] == category].shape[0] > count:\n",
    "            tmp = df[df['category'] == category].sample(count, random_state=42)\n",
    "        else:\n",
    "            tmp = df[df['category'] == category]\n",
    "            tmp = tmp.append(df[df['category'] == category].sample(count - df[df['category'] == category].shape[0], random_state=42, replace=True))\n",
    "        return tmp\n",
    "    tmp1 = equalize_category(dataset, 'info_news', count)\n",
    "    tmp2 = equalize_category(dataset, 'personal', count)\n",
    "    tmp3 = equalize_category(dataset, 'celebrity', count)\n",
    "    tmp4 = equalize_category(dataset, 'plan', count)\n",
    "    tmp5 = equalize_category(dataset, 'unrelated', count)\n",
    "    tmp6 = equalize_category(dataset, 'others', count)\n",
    "    tmp7 = equalize_category(dataset, 'requests', count)\n",
    "    tmp8 = equalize_category(dataset, 'rumors', count)\n",
    "    tmp9 = equalize_category(dataset, 'advice', count)\n",
    "    tmp10 = equalize_category(dataset, 'restrictions', count)\n",
    "    return pd.concat([tmp1, tmp2, tmp3, tmp4, tmp5, tmp6, tmp7, tmp8, tmp9, tmp10])\n",
    "\n",
    "# sample the train dataset such that every category contains 50 tweets\n",
    "train_5 = equalize_dataset_category(train, 50)\n",
    "\n",
    "# sample the train dataset such that every category contains 100 tweets\n",
    "train_6 = equalize_dataset_category(train, 100)\n",
    "\n",
    "# sample the train dataset such that every category contains 150 tweets\n",
    "train_7 = equalize_dataset_category(train, 150)\n",
    "\n",
    "# sample the train dataset such that every category contains 250 tweets\n",
    "train_8 = equalize_dataset_category(train, 250)\n",
    "\n",
    "# sample the train dataset such that every category contains 500 tweets\n",
    "\n",
    "train_9 = equalize_dataset_category(train, 500)\n",
    "\n",
    "# sample the train dataset such that every category contains 750 tweets\n",
    "train_10 = equalize_dataset_category(train, 750)\n",
    "\n",
    "# sample the train dataset such that every category contains 1000 tweets\n",
    "train_11 = equalize_dataset_category(train, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6988/6988 [00:00<00:00, 11574.01it/s]\n",
      "100%|██████████| 6988/6988 [00:01<00:00, 6695.59it/s]\n",
      "100%|██████████| 6988/6988 [00:01<00:00, 3877.65it/s]\n",
      "100%|██████████| 6988/6988 [00:01<00:00, 5165.38it/s]\n",
      "100%|██████████| 1500/1500 [00:00<00:00, 11662.05it/s]\n",
      "100%|██████████| 1500/1500 [00:00<00:00, 6661.08it/s]\n",
      "100%|██████████| 1500/1500 [00:00<00:00, 3779.41it/s]\n",
      "100%|██████████| 1500/1500 [00:00<00:00, 4992.85it/s]\n",
      "100%|██████████| 3000/3000 [00:00<00:00, 11843.84it/s]\n",
      "100%|██████████| 3000/3000 [00:00<00:00, 6276.97it/s]\n",
      "100%|██████████| 3000/3000 [00:00<00:00, 3821.47it/s]\n",
      "100%|██████████| 3000/3000 [00:00<00:00, 5011.55it/s]\n",
      "100%|██████████| 7500/7500 [00:00<00:00, 11454.80it/s]\n",
      "100%|██████████| 7500/7500 [00:01<00:00, 6350.18it/s]\n",
      "100%|██████████| 7500/7500 [00:01<00:00, 3794.02it/s]\n",
      "100%|██████████| 7500/7500 [00:01<00:00, 4793.25it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 11875.35it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 6636.26it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 3998.48it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 4889.37it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 11276.07it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 6548.79it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3909.17it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 4941.00it/s]\n",
      "100%|██████████| 1500/1500 [00:00<00:00, 11719.45it/s]\n",
      "100%|██████████| 1500/1500 [00:00<00:00, 5121.14it/s]\n",
      "100%|██████████| 1500/1500 [00:00<00:00, 4033.18it/s]\n",
      "100%|██████████| 1500/1500 [00:00<00:00, 5263.95it/s]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 11528.27it/s]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 6759.49it/s]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 3932.12it/s]\n",
      "100%|██████████| 2500/2500 [00:00<00:00, 5236.42it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 11735.64it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 6777.03it/s]\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 3933.57it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 5224.14it/s]\n",
      "100%|██████████| 7500/7500 [00:00<00:00, 11751.49it/s]\n",
      "100%|██████████| 7500/7500 [00:01<00:00, 6829.93it/s]\n",
      "100%|██████████| 7500/7500 [00:01<00:00, 4037.38it/s]\n",
      "100%|██████████| 7500/7500 [00:01<00:00, 5104.35it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 11515.57it/s]\n",
      "100%|██████████| 10000/10000 [00:01<00:00, 6567.12it/s]\n",
      "100%|██████████| 10000/10000 [00:02<00:00, 3966.27it/s]\n",
      "100%|██████████| 10000/10000 [00:01<00:00, 5122.70it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 11440.62it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 6510.03it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 3731.85it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 5169.52it/s]\n"
     ]
    }
   ],
   "source": [
    "train_1 = original_preprocess(train_1)\n",
    "train_2 = original_preprocess(train_2)\n",
    "train_3 = original_preprocess(train_3)\n",
    "train_4 = original_preprocess(train_4)\n",
    "train_5 = original_preprocess(train_5)\n",
    "train_6 = original_preprocess(train_6)\n",
    "train_7 = original_preprocess(train_7)\n",
    "train_8 = original_preprocess(train_8)\n",
    "train_9 = original_preprocess(train_9)\n",
    "train_10 = original_preprocess(train_10)\n",
    "train_11 = original_preprocess(train_11)\n",
    "dev_1 = original_preprocess(dev_1)\n",
    "\n",
    "# save the datasets to pickle files\n",
    "train_1.to_pickle('output/train_1_original.pkl')\n",
    "train_2.to_pickle('output/train_2_original.pkl')\n",
    "train_3.to_pickle('output/train_3_original.pkl')\n",
    "train_4.to_pickle('output/train_4_original.pkl')\n",
    "train_5.to_pickle('output/train_5_original.pkl')\n",
    "train_6.to_pickle('output/train_6_original.pkl')\n",
    "train_7.to_pickle('output/train_7_original.pkl')\n",
    "train_8.to_pickle('output/train_8_original.pkl')\n",
    "train_9.to_pickle('output/train_9_original.pkl')\n",
    "train_10.to_pickle('output/train_10_original.pkl')\n",
    "train_11.to_pickle('output/train_11_original.pkl')\n",
    "dev_1.to_pickle('output/dev_1_original.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (default, Sep  4 2020, 07:30:14) \n[GCC 7.3.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "0de4e91a703ed839300569d00f5f7f96e515c5b82ac00b296860b354eb12ca4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
