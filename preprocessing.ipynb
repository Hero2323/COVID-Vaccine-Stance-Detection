{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from tqdm import tqdm\n",
    "from arabert.preprocess import ArabertPreprocessor\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\swak\\anaconda3\\lib\\site-packages (from transformers) (1.20.1)\n",
      "Requirement already satisfied: requests in c:\\users\\swak\\anaconda3\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\swak\\anaconda3\\lib\\site-packages (from transformers) (4.59.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\swak\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp38-cp38-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\swak\\anaconda3\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\swak\\anaconda3\\lib\\site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\swak\\anaconda3\\lib\\site-packages (from transformers) (20.9)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Using cached huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\swak\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\swak\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\swak\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\swak\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\swak\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\swak\\anaconda3\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('dataset/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    5538\n",
       "1    1012\n",
       "0     438\n",
       "Name: stance, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['stance'] += 1\n",
    "train['stance'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Down sample class 1 to be 2500\n",
    "train_2 = train[train['stance'] == 2].sample(n=2500, random_state=42, replace=True)\n",
    "train_1 = train[train['stance'] == 1]\n",
    "train_0 = train[train['stance'] == 0]\n",
    "\n",
    "# Upsample classes 1 & 0 to be 2500\n",
    "train_1 = train_1.sample(n=2500, replace=True, random_state=42)\n",
    "train_0 = train_0.sample(n=2500, replace=True, random_state=42)\n",
    "\n",
    "train = pd.concat([train_0, train_1, train_2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-12-13 16:52:02,926 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n",
      "Downloading: 100%|██████████| 543M/543M [02:36<00:00, 3.48MB/s] \n",
      "c:\\Users\\swak\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\swak\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at aubmindlab/bert-base-arabertv2 were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 7500/7500 [10:51<00:00, 11.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing using arabert\n",
    "model_name = \"aubmindlab/bert-base-arabertv2\"\n",
    "arabert_prep = ArabertPreprocessor(model_name=model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "bert_model = BertModel.from_pretrained(model_name)\n",
    "# Can convert words to ids using: tokenizer.convert_tokens_to_ids()\n",
    "\n",
    "def preprocess_arabert(text, embedding=True):\n",
    "    \"\"\"\n",
    "    This function preprocesses the text using arabert.\n",
    "    It's essentially a full pipeline that even returns the word embeddings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text: str\n",
    "        The text to be preprocessed\n",
    "    embedding: bool\n",
    "        Whether to return the word embeddings or not\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    output: list\n",
    "        The preprocessed text\n",
    "    \"\"\"\n",
    "    def clean_text(text):\n",
    "        # remove any word with + in it\n",
    "        text = re.sub(r'\\S*\\+\\S*', '', text)\n",
    "        # remove non arabic characters\n",
    "        text = re.sub(r'[^\\u0600-\\u06FF]', ' ', text)\n",
    "        # remove extra spaces\n",
    "        return text\n",
    "    output = arabert_prep.preprocess(text)\n",
    "    output = clean_text(output)\n",
    "    tokenized = tokenizer.tokenize(output)\n",
    "    # pad all tokenized sentences to be the same length\n",
    "    max_len = 0\n",
    "    if embedding:\n",
    "        indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized)\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(tokens_tensor)\n",
    "            encoded_layers = outputs[0]\n",
    "        return encoded_layers\n",
    "    else:\n",
    "        return tokenized\n",
    "\n",
    "# preprocess the entire dataset\n",
    "tqdm.pandas()\n",
    "train_arabert = train['text'].progress_apply(preprocess_arabert)\n",
    "\n",
    "# This is for padding so that all sentences are the same length\n",
    "# I don't suggest doing this and saving the padded sentences because\n",
    "# the size goes from 550 MB to more than 2.5 GB, instead do this when training\n",
    "# pad_embedding = torch.zeros(1, 1, 768)\n",
    "# max_len = 0\n",
    "# for i in range(len(train_arabert)):\n",
    "#     if train_arabert[i].shape[1] > max_len:\n",
    "#         max_len = train_arabert[i].shape[1]\n",
    "# for i in range(len(train_arabert)):\n",
    "#     if train_arabert[i].shape[1] < max_len:\n",
    "#         pad = torch.zeros(1, max_len - train_arabert[i].shape[1], 768)\n",
    "#         train_arabert[i] = torch.cat((train_arabert[i], pad), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arabert.to_pickle('dataset/train_arabert_not_padded.pkl') # save the preprocessed dataset (550 MB) - put into git ignore\n",
    "# It's only here so that there's no need to do the preprocessing again when testing the code\n",
    "# train_arabert = pd.read_pickle('dataset/train_arabert_not_padded.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is made because the process of cleaning arabic text is complex\n",
    "# and depends on many unicodes done in many steps\n",
    "def clean_arabic(text):\n",
    "    # ! to understand this nonsense you need this link open \n",
    "    \"\"\" https://ar.wikipedia.org/wiki/%D8%A7%D9%84%D8%AE%D8%B7_\n",
    "        %D8%A7%D9%84%D8%B9%D8%B1%D8%A8%D9%8A_%D9%81%D9%8A_%D9%8A%D9\n",
    "        %88%D9%86%D9%8A%D9%83%D9%88%D8%AF   \"\"\"\n",
    "\n",
    "    # remove التشكيل\n",
    "    text = re.sub(r'[\\u0600-\\u061F]', '', text)\n",
    "    text = re.sub(r'[\\u064B-\\u066D]', '', text)\n",
    "\n",
    "    # Because of all the idiots that were bypassing twitters' spam filters using\n",
    "    # special characters like this idiot: كو.ر.ونا We'll remove all the special \n",
    "    # before everything else\n",
    "\n",
    "    # remove special characters\n",
    "    text = re.sub(r'[\\u0024-\\u003F]', '', text)\n",
    "    text = re.sub(r'[\\u005B-\\u0060]', '', text)\n",
    "    text = re.sub(r'[\\u007B-\\u007E]', '', text)\n",
    "\n",
    "    # replace weird characters with more standard ones\n",
    "    # 1. replace چ with ج\n",
    "    text = re.sub(r'چ','ج',text)\n",
    "\n",
    "    # 2. replace ڤ ڨ with ف\n",
    "    text = re.sub(r'ڤ','ف',text)\n",
    "    text = re.sub(r'ڨ','ف',text)\n",
    "\n",
    "    # 3. replace ڠ with ق\n",
    "    text = re.sub(r'ڠ','غ',text)\n",
    "    \n",
    "    # 4. replace ٱ\tٲ\tٳ\t◌ٴ\tٵ with ا\n",
    "    string = ['ٱ','ٲ','ٳ','ٴ','ٵ', 'آ', 'أ', 'إ']\n",
    "    for char in string:\n",
    "        text = re.sub(char,'ا',text)\n",
    "\n",
    "    # 5. replace ٶ\tٷ with و\n",
    "    string = ['ٶ','ٷ']\n",
    "    for char in string:\n",
    "        text = re.sub(char,'و',text)\n",
    "\n",
    "    # 6. replace ٸ ی with ي\n",
    "    text = re.sub(r'ٸ','ي',text) \n",
    "    text = re.sub(r'ی','ي',text)\n",
    "    \n",
    "    # 7. replace پ\twith ب\n",
    "    text = re.sub(r'پ','ب',text)\n",
    "\n",
    "    # 8. replace ژ with ز\n",
    "    text = re.sub(r'ژ','ز',text)\n",
    "\n",
    "    # 9. replace ک ڪ ګ ڬ ڭ ڮ گ ڰ ڱ ڲ ڳ ڴ with ك\n",
    "    string = ['ک', 'ڪ', 'ګ', 'ڬ', 'ڭ', 'ڮ', 'گ', 'ڰ', 'ڱ', 'ڲ', 'ڳ', 'ڴ']\n",
    "    for char in string:\n",
    "        text = re.sub(char,'ك',text)\n",
    "    # 10. replace ھ with ه\n",
    "    text = re.sub(r'ھ','ه',text)\n",
    "\n",
    "    # remove all extra arabic characters (shift + ت) \n",
    "    text = re.sub(r'ـ','',text)\n",
    "\n",
    "    # remove non arabic characters\n",
    "    text = re.sub(r'[^\\u0620-\\u064A\\s]',' ',text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \" removes all non arabic characters & replaces all spaces with a single space \"\n",
    "    # TODO: figure out how this works\n",
    "    # remove all non arabic characters & numbers\n",
    "    # text = re.sub(r'[^\\u0621-\\u064A\\u0660-\\u0669\\s]','',text)\n",
    "    \n",
    "    # remove all words with # in them\n",
    "    text = re.sub(r'[^\\s]*#[^\\s]*',' ',text)\n",
    "    \n",
    "    # arabic letters clean up \n",
    "    text = clean_arabic(text)\n",
    "        \n",
    "    # replace all white spaces with a single space\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    \n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_train_data = train['text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might need to download punkt & stopwords\n",
    "# ! Run the following lines in an empty cell if the code doesn't work\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# currently this line just tokenizes the text using a space (I think)\n",
    "tokenized_train_data = [nltk.word_tokenize(text) for text in cleaned_train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3185    الموت أرحم عندي من انو  آخد لقاح موهوب من دولة...\n",
       "3185    الموت أرحم عندي من انو  آخد لقاح موهوب من دولة...\n",
       "3185    الموت أرحم عندي من انو  آخد لقاح موهوب من دولة...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text'][3185]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('arabic')\n",
    "tokenized_no_stopwords_train_data = [[word for word in text if word not in stopwords] for text in tokenized_train_data]\n",
    "\n",
    "# Here we're looking for more stopwords that are 2 characters or less\n",
    "# we spend hours doing just this for two or three character words\n",
    "stopwords_extracted_from_dataset = ['ال', 'اي', 'ان', 'تم', 'بن', \n",
    "                                    'او', 'اي', 'عم', 'ام', 'رض',\n",
    "                                    'في', 'فى', 'رب', 'سم', 'خط',\n",
    "                                    'ول', 'زي', 'دي', 'اذ', 'ده',\n",
    "                                    'دى', 'انه', 'ابو', 'احد']\n",
    "\n",
    "tokenized_no_stopwords_train_data_v2 = [[word for word in text if word not in stopwords_extracted_from_dataset] for text in tokenized_no_stopwords_train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set = []\n",
    "for text in tokenized_no_stopwords_train_data_v2:\n",
    "    for word in text:\n",
    "        if len(word) <= 2:\n",
    "            stopwords_set.append((word, str(tokenized_no_stopwords_train_data_v2.index(text))))\n",
    "\n",
    "stopwords_set.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-e790e6043162>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mstemmed_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'stemmed_train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\swak\\anaconda3\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_SparseArray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 244\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"module 'pandas' has no attribute '{name}'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "stemmed_train = []\n",
    "\n",
    "pd.to_csv('stemmed_train.csv')\n",
    "\n",
    "\n",
    "for text in tqdm(tokenized_no_stopwords_train_data_v2):\n",
    "    curr_list = []\n",
    "    for word in text:\n",
    "        curr_list.append(ISRIStemmer().stem(word))\n",
    "    stemmed_train.append(curr_list)\n",
    "\n",
    "# stemmed_train = [[stemmer.stem(word) for word in text] for text in tokenized_no_stopwords_train_data_v2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write all of the texts into a file for visual comparison\n",
    "with open('output/original_train.txt', 'w') as f:\n",
    "    for text in train['text']:\n",
    "        f.write(text + '\\n\\n')\n",
    "f.close()\n",
    "\n",
    "with open('output/cleaned_up_train.txt', 'w') as f:\n",
    "    for text in cleaned_train_data:\n",
    "        f.write(text + '\\n\\n')\n",
    "f.close()\n",
    "\n",
    "with open('output/tokens_train.txt', 'w') as f:\n",
    "    for text in tokenized_train_data:\n",
    "        f.write(str(text) + '\\n\\n')\n",
    "f.close()\n",
    "\n",
    "with open('output/tokens_no_stopwords_train.txt', 'w') as f:\n",
    "    for text in tokenized_no_stopwords_train_data:\n",
    "        f.write(str(text) + '\\n\\n')\n",
    "f.close()\n",
    "\n",
    "with open('output/words_less_than_2.txt', 'w') as f:\n",
    "    for word in stopwords_set:\n",
    "        f.write(str(word) + '\\n')\n",
    "f.close()\n",
    "\n",
    "with open('output/stemmed_train.txt', 'w') as f:\n",
    "    for text in stemmed_train:\n",
    "        f.write(str(text) + '\\n\\n')\n",
    "f.close()\n",
    "# use alt + z to toggle word wrap in vscode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "655c12636761ad29ab72f84a68a8a53faa13a1d27df70400707a6bbc70c23f25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
